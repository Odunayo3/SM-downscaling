{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9212fd57-2182-4ebf-bd6c-f63b2825a79c",
   "metadata": {},
   "source": [
    "# **Soil Moisture Prediction and Downscaling with LightGBM**\n",
    "In this notebook, we use LightGBM to model soil moisture (ESACCI SM) with auxiliary datasets, perform post hoc interpretation, evaluate model residuals, and conduct spatial prediction for downscaling at 1 km resolution.\n",
    "---\n",
    "\n",
    "## Step 1: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b822a2-d34b-494d-9a6f-8217c617a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from hyperopt import hp, tpe, fmin, Trials, space_eval\n",
    "from scipy.stats import uniform, loguniform, randint\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import dalex as dx\n",
    "import glob\n",
    "import rasterio\n",
    "import os\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add40889-052e-42bb-8f51-23c57dc3f0aa",
   "metadata": {},
   "source": [
    "## Step 2: Data Loading and Preprocessing\n",
    "We load the ESA CCI dataset and convert categorical features to the 'category' data type for compatibility with LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c01a45-b41a-4ec1-ba77-32e2995aa267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESA CCI soil moisture dataset\n",
    "data = pd.read_csv(\"/home/eoafrica/...\")\n",
    "\n",
    "# Convert categorical columns to type 'category'\n",
    "data['Landcover'] = data['Landcover'].astype('category')\n",
    "data['soil_text'] = data['soil_text'].astype('category')\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop('ESACCI', axis=1)  # ESACCI is our target column\n",
    "y = data['ESACCI']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560dec6-587b-404b-8162-334cba56b22a",
   "metadata": {},
   "source": [
    "## Step 3: Stratified Sampling for Train, Validation, and Test Sets\n",
    "To maintain representativeness across categories, we use StratifiedShuffleSplit to divide data into training, validation, and testing subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198558fb-33b2-44df-baef-4bb8712c00cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified sampling based on categorical features\n",
    "stratified_splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, remaining_index in stratified_splitter.split(X, X[['soil_text', 'Landcover']]):\n",
    "    X_train, X_remaining = X.iloc[train_index], X.iloc[remaining_index]\n",
    "    y_train, y_remaining = y.iloc[train_index], y.iloc[remaining_index]\n",
    "\n",
    "stratified_splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "for valid_index, test_index in stratified_splitter.split(X_remaining, X_remaining[['soil_text', 'Landcover']]):\n",
    "    X_valid, X_test = X_remaining.iloc[valid_index], X_remaining.iloc[test_index]\n",
    "    y_valid, y_test = y_remaining.iloc[valid_index], y_remaining.iloc[test_index]\n",
    "\n",
    "# Define categorical features for LightGBM\n",
    "categorical_features = ['soil_text', 'Landcover']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da6d507-4e2b-49fc-b5f3-e6b653a9a2db",
   "metadata": {},
   "source": [
    "## Step 4: Hyperparameter Tuning Using Optuna\n",
    "Here, we define an objective function for optimizing LightGBM hyperparameters using Optuna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febef02e-30d6-4436-ac2d-9a16ded940aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Define objective function to minimize\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 300, 500, step=50)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.1, log=True)\n",
    "    subsample = trial.suggest_float('subsample', 0.7, 1.0)\n",
    "    num_leaves = trial.suggest_int('num_leaves', 150, 255)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "    min_split_gain = trial.suggest_float('min_split_gain', 0.001, 0.1, log=True)\n",
    "    min_child_samples = trial.suggest_int('min_child_samples', 20, 80, step=10)\n",
    "\n",
    "    # Train model with current hyperparameters\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective='regression',\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        subsample=subsample,\n",
    "        min_split_gain=min_split_gain,\n",
    "        num_leaves=num_leaves,\n",
    "        min_child_samples=min_child_samples,\n",
    "        force_col_wise=True,\n",
    "        verbosity=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric='rmse',\n",
    "        categorical_feature=categorical_features\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    rsquare = r2_score(y_valid, y_pred)\n",
    "\n",
    "    # Clean up memory\n",
    "    del y_pred\n",
    "    gc.collect()\n",
    "    return -rsquare\n",
    "\n",
    "# Run the optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c07e6-2c4d-472b-b0ea-a6dd1d4407ef",
   "metadata": {},
   "source": [
    "## Step 5: Model Training with Best Hyperparameters\n",
    "Using the optimal hyperparameters identified, we train the final LightGBM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10740f74-bbc8-4057-88c4-88b3b1f2e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "lgb_model = lgb.LGBMRegressor(**best_params, force_col_wise=True, verbosity=-1, random_state=42)\n",
    "lgb_model.fit(X_train, y_train, categorical_feature=categorical_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283be3bc-80e5-4246-bf3a-bedbe9d08885",
   "metadata": {},
   "source": [
    "## Step 6: Model Evaluation Metrics (RMSE, CCC, Bias)\n",
    "We calculate Root Mean Squared Error (RMSE), Concordance Correlation Coefficient (CCC), and Bias to evaluate the model's predictive performance across train, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d1133f-6b9b-4866-86d4-3bf48fa6ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric functions\n",
    "def calculate_ccc(y_true, y_pred):\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    covariance = np.mean((y_true - mean_true) * (y_pred - mean_pred))\n",
    "    std_true = np.std(y_true)\n",
    "    std_pred = np.std(y_pred)\n",
    "    ccc = 2 * covariance / (std_true**2 + std_pred**2 + (mean_true - mean_pred)**2)\n",
    "    return ccc\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_test = lgb_model.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "ccc_test = calculate_ccc(y_test, y_pred_test)\n",
    "\n",
    "print(\"Test RMSE:\", rmse_test)\n",
    "print(\"Test CCC:\", ccc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb95885-ab00-4b1b-9a43-0c73774febda",
   "metadata": {},
   "source": [
    "## Step 7: Visualization\n",
    "We plot observed vs. predicted values for train, validation, and test sets with density scatter plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a40e9-e811-41d8-a45d-86ea6d4b7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "fig, axs = plt.subplots(1, 3, figsize=(24, 8))\n",
    "font_properties = {'fontsize': 18, 'fontweight': 'bold'}\n",
    "\n",
    "# Define function for density scatter plot\n",
    "def density_scatter(x, y, ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    scatter = ax.scatter(x, y, c=np.histogram2d(x, y, bins=50, density=True)[0], **kwargs)\n",
    "    ax.plot([min(x), max(x)], [min(x), max(x)], color='black', linestyle='--')\n",
    "    return ax\n",
    "\n",
    "# Training plot\n",
    "density_scatter(y_train, lgb_model.predict(X_train), ax=axs[0], alpha=0.5)\n",
    "axs[0].set_title(\"Training Subset\", **font_properties)\n",
    "\n",
    "# Validation plot\n",
    "density_scatter(y_valid, lgb_model.predict(X_valid), ax=axs[1], alpha=0.5)\n",
    "axs[1].set_title(\"Validation Subset\", **font_properties)\n",
    "\n",
    "# Testing plot\n",
    "density_scatter(y_test, y_pred_test, ax=axs[2], alpha=0.5)\n",
    "axs[2].set_title(\"Testing Subset\", **font_properties)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bb222-9ace-4c6a-89a4-cf153d739956",
   "metadata": {},
   "source": [
    "## Step 8: Model Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a4ba2-44bf-4fa1-b19c-0d28ff0a8409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on training data\n",
    "y_pred_lgb = pd.Series(lgb_model.predict(X.drop(['Date', 'Lon', 'Lat'], axis=1)), index=X.index, name='pred_sm25km_lgb')\n",
    "residual_lgb = y - y_pred_lgb\n",
    "# Concatenate the datasets for plotting\n",
    "df = pd.concat([X, y, y_pred_lgb.rename('LGB_predictedsm_25km'), residual_lgb.rename('LGB_Residuals')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81979e64-fed0-4d40-b2a9-1ee4f68cadb5",
   "metadata": {},
   "source": [
    "## Step 9: SHAP Interpretation\n",
    "Using SHAP, we interpret feature importance and their impact on predictions through a summary plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b7da04-0f95-4c09-b637-146c3c6975ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP interpretation\n",
    "explainer = shap.Explainer(lgb_model)\n",
    "shap_values = explainer(X_train)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890f030b-eecc-4b59-8699-29c73060f57c",
   "metadata": {},
   "source": [
    "## Step 10: Partial Dependence Plots with DALEX\n",
    "To analyze feature impact, we create partial dependence plots for each feature using DALEX.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ee467-5228-4f50-ac06-5d2b94a38da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = dx.Explainer(lgb_model, X_train, y_train, verbose=False)\n",
    "exp.model_profile(type='partial', random_state=42, N=20000).plot(y_title=\"Average Soil Moisture Predicted\", title=\"Partial Dependence Plots\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69ee66-74e5-432b-9176-53fa2d5d54d0",
   "metadata": {},
   "source": [
    "## Step 11: Spatial Prediction and Downscaling\n",
    "Finally, we predict soil moisture across a 1 km resolution spatial grid by reading auxiliary datasets and saving the downscaled results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef64b3c-2feb-40ac-ae9f-ee4129b2b950",
   "metadata": {},
   "source": [
    "# # Soil Moisture Prediction Pipeline with Raster Data Integration\n",
    "# This notebook processes multiple raster datasets, applies transformations, and generates soil moisture (SM) predictions using a LightGBM model. Each prediction is saved as a TIFF raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73874b0-b877-4b86-ac8d-76079c254f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 1: Import Necessary Libraries\n",
    "import os\n",
    "import glob\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f4147-8d03-49d6-b796-aca3dd3a43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 2: Define Utility Functions for Raster Handling\n",
    "def load_raster(file_path):\n",
    "    \"\"\"Load a raster file and return the data array, transform, and CRS.\"\"\"\n",
    "    with rasterio.open(file_path) as src:\n",
    "        return src.read(1), src.transform, src.crs\n",
    "\n",
    "def save_raster(data, transform, crs, output_file):\n",
    "    \"\"\"Save data to a specified file path as a GeoTIFF raster.\"\"\"\n",
    "    with rasterio.open(\n",
    "        output_file, 'w', driver='GTiff', height=data.shape[0],\n",
    "        width=data.shape[1], count=1, dtype=data.dtype,\n",
    "        crs=crs, transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(data, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba531b17-c551-4ff0-bfda-63a9f0e52dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 3: Define Input and Output Directories\n",
    "# Specify the directory paths for input covariates and output results.\n",
    "ndvi_dir = \"/home/eoafrica/Satellite_sm/downscaling/Covariates/NDVI_pred\"\n",
    "lst_dir = \"/home/eoafrica/Satellite_sm/downscaling/Covariates/blended_LST\"\n",
    "TVDI_folder = '/home/eoafrica/Satellite_sm/downscaling/Covariates/TVDI'\n",
    "ESACCI_folder = \"/home/eoafrica/Satellite_sm/downscaling/Reproj/ESACCI_raster/ESACCI_adj/\"\n",
    "Covariates_folder = \"/home/eoafrica/Satellite_sm/downscaling/Covariates/\"\n",
    "precip_folder = \"/home/eoafrica/Satellite_sm/downscaling/Covariates/precipitation\"\n",
    "output_dir = \"/home/eoafrica/Satellite_sm/downscaling/Spatial maps\"\n",
    "temp_folder = \"/home/eoafrica/Satellite_sm/downscaling/temp_dir\"\n",
    "skin_temp_folder = \"/home/eoafrica/Satellite_sm/downscaling/Covariates/ERA5\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba24f6-fdcc-42c4-b516-0264fad98539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 4: Prepare NDVI Files and Define Mapping Dictionaries\n",
    "# Gather NDVI files and define land cover and soil texture mappings for later use.\n",
    "ndvi_files = glob.glob(os.path.join(ndvi_dir, 'NDVI_*.tif'))\n",
    "\n",
    "landcover_mapping = {\n",
    "    1: 'Evergreen Needleleaf Forest', 2: 'Evergreen Broadleaf Forest', 3: 'Deciduous Needleleaf Forest',\n",
    "    4: 'Deciduous Broadleaf Forest', 5: 'Mixed Forest', 6: 'Closed Shrublands', 7: 'Open Shrublands',\n",
    "    8: 'Woody Savannas', 9: 'Savannas', 10: 'Grasslands', 11: 'Permanent Wetlands', 12: 'Croplands',\n",
    "    13: 'Urban', 14: 'Cropland/Natural Vegetation', 16: 'Barren land'\n",
    "}\n",
    "\n",
    "soil_text_mapping = {\n",
    "    1: 'Cl', 2: 'SiCl', 3: 'SaCl', 4: 'ClLo', 5: 'SiClLo', 6: 'SaClLo', 7: 'Lo', 8: 'SiLo', 9: 'SaLo',\n",
    "    10: 'Si', 11: 'LoSa', 12: 'Sa'\n",
    "}\n",
    "\n",
    "# Vectorize the mapping functions\n",
    "landcover_vectorized = np.vectorize(landcover_mapping.get)\n",
    "soil_text_vectorized = np.vectorize(soil_text_mapping.get)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50340d1a-ed65-4033-9c4a-487b48dd1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 5: Loop Through Each NDVI File for Processing\n",
    "# For each date-specific NDVI file, match covariates, apply transformations, and make predictions.\n",
    "for ndvi_file in ndvi_files:\n",
    "    date = os.path.basename(ndvi_file).split('_')[1].split('.')[0]\n",
    "\n",
    "    # Generate paths for the corresponding date\n",
    "    skin_temp_file = os.path.join(skin_temp_folder, f\"skin_temperature_{date}.tif\")\n",
    "    precip_file = os.path.join(precip_folder, f\"precipitation_{date}.tif\")\n",
    "    tvdi_file = os.path.join(TVDI_folder, f\"TVDI_{date}.tif\")\n",
    "    lst_file = os.path.join(lst_dir, f\"LST_{date}.tif\")\n",
    "    lulc_file = os.path.join(Covariates_folder, f\"LULC_{date[0:4]}.tif\")\n",
    "    \n",
    "    # Check if all required files are present\n",
    "    if not (os.path.exists(ndvi_file) and os.path.exists(skin_temp_file) and os.path.exists(precip_file) and os.path.exists(tvdi_file) and os.path.exists(lst_file) and os.path.exists(lulc_file)):\n",
    "        continue\n",
    "    \n",
    "    # Load each raster\n",
    "    ndvi, transform, crs = load_raster(ndvi_file)\n",
    "    skin_temp, _, _ = load_raster(skin_temp_file)\n",
    "    tvdi, _, _ = load_raster(tvdi_file)\n",
    "    precip, _, _ = load_raster(precip_file)\n",
    "    lst, _, _ = load_raster(lst_file)\n",
    "    lulc, _, _ = load_raster(lulc_file)\n",
    "\n",
    "    # Remove specified values from LULC by setting them to NaN\n",
    "    lulc = np.where((lulc == 17) | (lulc == 255), np.nan, lulc)\n",
    "    \n",
    "    # Flatten LULC and apply mapping\n",
    "    lulc_flat = lulc.flatten()\n",
    "    valid_mask = ~np.isnan(lulc_flat)\n",
    "    lulc_e_flat = np.full(lulc_flat.shape, np.nan, dtype=object)\n",
    "    lulc_e_flat[valid_mask] = landcover_vectorized(lulc_flat[valid_mask])\n",
    "    lulc = lulc_e_flat.reshape(lulc.shape)\n",
    "\n",
    "    # ## Step 6: Load Static Covariates\n",
    "    # Load elevation, soil bulk density, and soil texture rasters.\n",
    "    elevation, _, _ = load_raster(os.path.join(Covariates_folder, 'Elevation.tif'))\n",
    "    soil_bulk, _, _ = load_raster(os.path.join(Covariates_folder, 'soil_bulkdensity.tif'))\n",
    "    soil_text, _, _ = load_raster(os.path.join(Covariates_folder, 'soil_texture.tif'))\n",
    "\n",
    "    # Apply soil texture mapping similarly to LULC\n",
    "    soil_text_flat = soil_text.flatten()\n",
    "    valid_soil_text_mask = ~np.isnan(soil_text_flat)\n",
    "    soil_text_e_flat = np.full(soil_text_flat.shape, np.nan, dtype=object)\n",
    "    soil_text_e_flat[valid_soil_text_mask] = soil_text_vectorized(soil_text_flat[valid_soil_text_mask])\n",
    "    soil_text = soil_text_e_flat.reshape(soil_text.shape)\n",
    "    \n",
    "    # ## Step 7: Stack Covariates and Apply Mask\n",
    "    covariates_stack = np.stack([lst, ndvi, tvdi, precip, elevation, soil_bulk, skin_temp, lulc, soil_text], axis=-1)\n",
    "\n",
    "    numeric_mask = (elevation < 0) | (ndvi < 0)\n",
    "    object_mask = pd.isna(lulc) | pd.isna(soil_text)\n",
    "    combined_mask = numeric_mask | object_mask\n",
    "    covariates_stack = np.where(combined_mask[..., None], np.nan, covariates_stack)\n",
    "\n",
    "    # ## Step 8: Reshape and Create DataFrame for Prediction\n",
    "    n_rows, n_cols, n_features = covariates_stack.shape\n",
    "    covariates_stack_reshaped = covariates_stack.reshape(-1, n_features)\n",
    "    df_covariates = pd.DataFrame(covariates_stack_reshaped, columns=['lst', 'ndvi', 'tvdi', 'precip', 'elevation', 'soil_bulk', 'skin_temp', 'lulc', 'soil_text'])\n",
    "    \n",
    "    # Convert numeric columns to float32\n",
    "    numeric_columns = ['lst', 'ndvi', 'tvdi', 'precip', 'elevation', 'soil_bulk', 'skin_temp']\n",
    "    df_covariates[numeric_columns] = df_covariates[numeric_columns].astype('float32')\n",
    "    \n",
    "    # Ensure categorical columns are set as categories\n",
    "    df_covariates['lulc'] = df_covariates['lulc'].astype('category')\n",
    "    df_covariates['soil_text'] = df_covariates['soil_text'].astype('category')\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    valid_covariates = df_covariates.dropna()\n",
    "\n",
    "    # ## Step 9: Predict Soil Moisture and Save Results\n",
    "    sm_predictions = np.full((n_rows * n_cols,), np.nan)\n",
    "    sm_predictions[valid_covariates.index] = lgb_model.predict(valid_covariates)\n",
    "    sm_predictions_reshaped = sm_predictions.reshape(n_rows, n_cols)\n",
    "    output_file = os.path.join(output_dir, f\"SM_{date}.tif\")\n",
    "    save_raster(sm_predictions_reshaped, transform, crs, output_file)\n",
    "    \n",
    "    print(f\"Predicted and saved Soil Moisture for {date}\")\n",
    "\n",
    "print(\"All predictions completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
